# Q-Learning Agent para FrozenLake-v1

Este m√≥dulo implementa un agente de Q-Learning tabular completo para el entorno FrozenLake-v1 (4x4) de Gymnasium, siguiendo la Parte 2 del TP de DQN.

## üéØ Caracter√≠sticas Implementadas

### ‚úÖ Clase QLearningAgent
- **Tabla Q**: Implementada con `defaultdict` para manejo autom√°tico de estados no visitados
- **Pol√≠tica epsilon-greedy**: Balanceo entre exploraci√≥n y explotaci√≥n
- **Actualizaci√≥n Q-Learning**: Regla est√°ndar `Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_a' Q[s',a'] ‚àí Q(s,a)]`
- **Decaimiento exponencial de epsilon**: `Œµ = min_epsilon + (max_epsilon - min_epsilon) * exp(-decay_rate * episode)`
- **Guardado/carga de modelos**: Serializaci√≥n con pickle

### ‚úÖ Funci√≥n de Entrenamiento (`train_qlearning`)
- **Entorno**: FrozenLake-v1 (4x4) con superficie deslizante
- **Hiperpar√°metros por defecto**:
  - `n_episodes = 5000`
  - `alpha = 0.8` (tasa de aprendizaje)
  - `gamma = 0.99` (factor de descuento)
  - `epsilon_min = 0.1`
  - `decay_rate = 0.001`
- **Tracking de progreso**: Recompensas por episodio y promedios cada 100 episodios
- **Progreso visual**: Barra de progreso con `tqdm`

### ‚úÖ Funci√≥n de Evaluaci√≥n (`evaluate_qlearning`)
- **Evaluaci√≥n sin aprendizaje**: 100 episodios por defecto
- **Pol√≠tica configurable**: Greedy (Œµ=0.0) o epsilon-greedy
- **M√©tricas**: Tasa de √©xito (porcentaje de llegadas a la meta)

### ‚úÖ Visualizaci√≥n (`plot_training_curves`)
- **Gr√°fico de evoluci√≥n**: Recompensa promedio vs episodios
- **L√≠nea de tendencia suavizada**: Ventana m√≥vil para mejor visualizaci√≥n
- **Guardado autom√°tico**: En `plots/qlearning_rewards.png`
- **Informaci√≥n adicional**: Rendimiento final en el gr√°fico

## üìä Resultados T√≠picos

Con los hiperpar√°metros por defecto, el agente t√≠picamente alcanza:
- **Tasa de √©xito**: 20-30% (FrozenLake es un entorno desafiante)
- **Convergencia**: Alrededor de 2000-3000 episodios
- **Epsilon final**: ~0.1 (valor m√≠nimo configurado)

## üöÄ Uso

### Ejecuci√≥n B√°sica
```bash
# Entrenar y evaluar con configuraci√≥n por defecto
uv run qlearning/qlearning_agent.py
```

### Uso Program√°tico
```python
from qlearning.qlearning_agent import train_qlearning, evaluate_qlearning, plot_training_curves

# Entrenar agente
q_table, rewards, avg_rewards = train_qlearning(
    n_episodes=5000,
    alpha=0.8,
    gamma=0.99,
    epsilon_min=0.1,
    decay_rate=0.001
)

# Evaluar agente
success_rate = evaluate_qlearning(q_table, n_eval_episodes=100)
print(f"Tasa de √©xito: {success_rate*100:.2f}%")

# Generar gr√°ficos
plot_training_curves(avg_rewards)
```

### Demo Interactivo
```bash
# Ejecutar demo con m√∫ltiples opciones
uv run qlearning/demo_qlearning.py
```

## üìÅ Archivos

- **`qlearning_agent.py`**: Implementaci√≥n principal del agente Q-Learning
- **`demo_qlearning.py`**: Script de demostraci√≥n interactivo con comparaciones
- **`README.md`**: Esta documentaci√≥n

## üîß Hiperpar√°metros

### Par√°metros Principales
- **`alpha` (learning_rate)**: Controla qu√© tan r√°pido aprende el agente
  - Alto (0.8-0.9): Aprendizaje r√°pido pero potencialmente inestable
  - Bajo (0.1-0.3): Aprendizaje lento pero m√°s estable
  
- **`gamma` (discount_factor)**: Importancia de recompensas futuras
  - Alto (0.99): Considera recompensas a largo plazo
  - Bajo (0.9): Se enfoca en recompensas inmediatas
  
- **`decay_rate`**: Velocidad de decaimiento de epsilon
  - Alto (0.005): Transici√≥n r√°pida a explotaci√≥n
  - Bajo (0.0005): Exploraci√≥n prolongada

### Configuraciones Recomendadas

**Para convergencia r√°pida:**
```python
alpha=0.9, gamma=0.99, decay_rate=0.005
```

**Para estabilidad:**
```python
alpha=0.3, gamma=0.95, decay_rate=0.001
```

**Para exploraci√≥n extendida:**
```python
alpha=0.5, gamma=0.99, decay_rate=0.0005
```

## üìà An√°lisis de Resultados

El script genera autom√°ticamente:

1. **Gr√°fico de entrenamiento**: Evoluci√≥n de la recompensa promedio
2. **M√©tricas de rendimiento**: Tasa de √©xito final
3. **Informaci√≥n de convergencia**: Epsilon final y estad√≠sticas

Los gr√°ficos se guardan en la carpeta `plots/` y son ideales para incluir en informes acad√©micos.

## üéÆ Sobre FrozenLake-v1

FrozenLake es un entorno de navegaci√≥n en cuadr√≠cula donde:
- **Objetivo**: Llegar desde el inicio (S) hasta la meta (G)
- **Obst√°culos**: Hoyos (H) que terminan el episodio sin recompensa
- **Superficie deslizante**: Las acciones tienen probabilidad de fallar
- **Recompensa**: +1 solo al llegar a la meta, 0 en otros casos

### Mapa 4x4
```
SFFF
FHFH
FFFH
HFFG
```

Donde:
- S: Inicio (Start)
- F: Superficie congelada (Frozen)
- H: Hoyo (Hole)
- G: Meta (Goal)

## üîç Debugging y An√°lisis

El m√≥dulo incluye herramientas para analizar el comportamiento del agente:

- **Visualizaci√≥n de tabla Q**: Ver valores aprendidos para estados clave
- **Comparaci√≥n de hiperpar√°metros**: Evaluar diferentes configuraciones
- **An√°lisis de pol√≠tica**: Observar las acciones preferidas por estado

Esto facilita la comprensi√≥n del proceso de aprendizaje y la optimizaci√≥n de hiperpar√°metros.
