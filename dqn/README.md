# DQN (Deep Q-Network) Implementation

Este directorio contiene la implementaciÃ³n completa del algoritmo Deep Q-Network (DQN) siguiendo estrictamente el **Algoritmo 2** del paper original de Mnih et al. (2015).

## ğŸ¯ Archivos Principales

### `dqn_agent.py`
ImplementaciÃ³n del agente DQN con:
- **Arquitectura MLP** para CartPole (Linear 64 â†’ ReLU â†’ Linear 64 â†’ ReLU â†’ Linear output)
- **Arquitectura CNN** para Breakout (basada en el paper original de DQN)
- **Red online y target** con hard updates cada N steps
- **Replay buffer circular** para estabilizar el entrenamiento
- **PolÃ­tica epsilon-greedy** con decaimiento exponencial
- **MSE loss** para entrenamiento (como en el paper original)
- **Optimizer Adam** con learning rate configurable

### `train_cartpole.py`
Script de entrenamiento para CartPole-v1:
- **ParÃ¡metros tÃ­picos**: 1000 episodios, Î³=0.99, lr=1e-3, buffer=50k
- **Criterio de "resuelto"**: promedio â‰¥195 en 100 episodios consecutivos
- **Logging completo** a TensorBoard
- **Guardado automÃ¡tico** del mejor modelo

### `train_breakout.py`
Script de entrenamiento para Breakout-v5:
- **Preprocesamiento de imÃ¡genes**: 84x84 escala de grises
- **Frame stacking**: 4 frames consecutivos
- **ParÃ¡metros para Atari**: buffer=500k, start_learning=50k, lr=2.5e-4
- **Reward clipping**: [-1, +1] como en el paper
- **Life loss detection**: pÃ©rdida de vida como episodio terminado

## ğŸš€ Uso RÃ¡pido

### Entrenar en CartPole
```bash
# Entrenamiento bÃ¡sico
uv run dqn/train_cartpole.py

# Con parÃ¡metros personalizados
uv run dqn/train_cartpole.py --episodes 1500 --lr 0.001 --epsilon-decay 0.002

# Evaluar modelo entrenado
uv run dqn/train_cartpole.py --eval --render
```

### Entrenar en Breakout
```bash
# Entrenamiento bÃ¡sico (Â¡puede tardar horas!)
uv run dqn/train_breakout.py --episodes 1000

# Con parÃ¡metros personalizados
uv run dqn/train_breakout.py --episodes 2000 --lr 0.0001

# Evaluar modelo entrenado
uv run dqn/train_breakout.py --eval --render
```

## ğŸ“Š TensorBoard

### CÃ³mo Lanzar TensorBoard

Para visualizar los logs de entrenamiento en tiempo real:

```bash
# Desde el directorio raÃ­z del proyecto
tensorboard --logdir runs

# Para un entorno especÃ­fico
tensorboard --logdir runs/cartpole
tensorboard --logdir runs/breakout

# Con puerto personalizado
tensorboard --logdir runs --port 6007
```

Luego abre tu navegador en: `http://localhost:6006`

### MÃ©tricas Registradas

El sistema registra automÃ¡ticamente las siguientes mÃ©tricas:

#### ğŸ“ˆ **MÃ©tricas de Entrenamiento**
- **`train/episode_reward`**: Recompensa total por episodio
- **`train/avg_reward_100`**: Promedio mÃ³vil de recompensas (ventana 100)
- **`train/loss`**: PÃ©rdida MSE del entrenamiento
- **`train/epsilon`**: Valor actual de epsilon (exploraciÃ³n)

#### ğŸ® **MÃ©tricas por Entorno**

**CartPole-v1:**
- Objetivo: Mantener el poste equilibrado
- Recompensa mÃ¡xima por episodio: 500
- "Resuelto" cuando avg_reward_100 â‰¥ 195

**Breakout-v5:**
- Objetivo: Romper todos los ladrillos
- Recompensa variable segÃºn ladrillos rotos
- "Resuelto" cuando avg_reward_100 â‰¥ 30

### Estructura de Logs

```
runs/
â”œâ”€â”€ cartpole/
â”‚   â””â”€â”€ 20241213_143022/  # Timestamp del entrenamiento
â”‚       â”œâ”€â”€ events.out.tfevents.xxx
â”‚       â””â”€â”€ ...
â””â”€â”€ breakout/
    â””â”€â”€ 20241213_150045/
        â”œâ”€â”€ events.out.tfevents.xxx
        â””â”€â”€ ...
```

## ğŸ”§ ConfiguraciÃ³n de HiperparÃ¡metros

### CartPole (MLP)
```python
# ConfiguraciÃ³n por defecto
episodes = 1000
gamma = 0.99
lr = 1e-3
buffer_capacity = 50000
epsilon_start = 1.0
epsilon_min = 0.05
epsilon_decay = 0.001
batch_size = 32
target_update_freq = 1000
start_learning = 1000
```

### Breakout (CNN)
```python
# ConfiguraciÃ³n por defecto
episodes = 1000
gamma = 0.99
lr = 2.5e-4  # MÃ¡s bajo para Atari
buffer_capacity = 500000  # MÃ¡s grande
epsilon_start = 1.0
epsilon_min = 0.1  # MÃ¡s alto para Atari
epsilon_decay = 0.0001  # MÃ¡s lento
batch_size = 32
target_update_freq = 10000  # Menos frecuente
start_learning = 50000  # Mucho mÃ¡s alto
```

## ğŸ“ Modelos Guardados

Los modelos entrenados se guardan automÃ¡ticamente en:

```
checkpoints/
â”œâ”€â”€ dqn_cartpole.pt    # Mejor modelo de CartPole
â””â”€â”€ dqn_breakout.pt    # Mejor modelo de Breakout
```

### Cargar y Usar Modelos

```python
from dqn.dqn_agent import DQNAgent

# Crear agente
agent = DQNAgent(state_size=4, action_size=2, model_type="mlp")

# Cargar modelo entrenado
agent.load_model("checkpoints/dqn_cartpole.pt")

# Usar para predicciÃ³n
action = agent.select_action(state, epsilon=0.0)  # PolÃ­tica greedy
```

## ğŸ§ª Experimentos y Comparaciones

### Comparar con REINFORCE
```bash
# Generar comparaciÃ³n DQN vs REINFORCE
uv run experiments/compare_dqn_vs_reinforce.py \
    --dqn-tensorboard runs/cartpole/20241213_143022 \
    --reinforce-data path/to/reinforce_results.json \
    --output-dir plots
```

### AnÃ¡lisis de Resultados

Los scripts generan automÃ¡ticamente:
1. **GrÃ¡ficos de entrenamiento** (recompensa vs episodios)
2. **AnÃ¡lisis de eficiencia** (recompensa vs tiempo)
3. **Comparaciones** entre algoritmos
4. **Reportes de texto** con estadÃ­sticas

## ğŸ› Troubleshooting

### Problemas Comunes

**1. CUDA out of memory (Breakout)**
```bash
# Reducir batch size
uv run dqn/train_breakout.py --batch-size 16

# O usar CPU
CUDA_VISIBLE_DEVICES="" uv run dqn/train_breakout.py
```

**2. TensorBoard no muestra datos**
```bash
# Verificar que los logs existen
ls -la runs/

# Lanzar con verbose
tensorboard --logdir runs --verbose
```

**3. Entrenamiento muy lento**
```bash
# Verificar que usa GPU
python -c "import torch; print(torch.cuda.is_available())"

# Reducir frecuencia de logging
# (modificar cÃ³digo para log cada N episodios)
```

### Logs de Debug

Para debug detallado, activar logging en el cÃ³digo:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“š Referencias

- **Paper Original**: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) (Mnih et al., 2015)
- **Gymnasium**: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)
- **PyTorch**: [https://pytorch.org/](https://pytorch.org/)
- **TensorBoard**: [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard)

## ğŸ“ Notas AcadÃ©micas

Esta implementaciÃ³n sigue **estrictamente el Algoritmo 2** del paper original:
- âœ… Experience replay con buffer circular
- âœ… Target network con hard updates
- âœ… Epsilon-greedy con decaimiento exponencial
- âœ… MSE loss (no Huber por defecto)
- âœ… Adam optimizer
- âœ… Preprocesamiento estÃ¡ndar para Atari

**Diferencias con implementaciones modernas:**
- No usa Double DQN, Dueling DQN, o Prioritized Replay (extensiones posteriores)
- No usa Huber loss (aunque es comÃºn en implementaciones modernas)
- Frame stacking manual (no usa wrappers de Gymnasium)

Esto es intencional para mantener fidelidad al algoritmo original del paper.
