# TP Deep Q-Network (DQN) - Aprendizaje por Refuerzo

Este proyecto implementa algoritmos de aprendizaje por refuerzo, incluyendo Q-Learning tradicional y Deep Q-Networks (DQN), para resolver diferentes entornos de control.

## Estructura del Proyecto

```
TP2_RL/
â”œâ”€â”€ ğŸ“„ main.py                          # Punto de entrada principal con CLI
â”œâ”€â”€ ğŸ“„ pyproject.toml                   # ConfiguraciÃ³n del proyecto con uv
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Dependencias pip (legacy)
â”œâ”€â”€ ğŸ“„ README.md                        # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_SUMMARY.md        # Resumen de implementaciÃ³n completa
â”œâ”€â”€ ğŸ“„ INFORME_COMPLETO.md              # Informe detallado con resultados
â”œâ”€â”€ ğŸ“„ TP2 DQN-1.2.pdf                 # Enunciado del trabajo prÃ¡ctico
â”‚
â”œâ”€â”€ ğŸ“ envs/                            # Entornos personalizados para testing
â”‚   â”œâ”€â”€ constant_env.py                 # Entorno con recompensa constante (+1)
â”‚   â”œâ”€â”€ random_obs_env.py               # Entorno con observaciones aleatorias
â”‚   â””â”€â”€ two_step_env.py                 # Entorno minimal de dos estados
â”‚
â”œâ”€â”€ ğŸ“ qlearning/                       # ImplementaciÃ³n Q-Learning tabular
â”‚   â”œâ”€â”€ qlearning_agent.py              # Agente Q-Learning con tabla Q
â”‚   â”œâ”€â”€ train_custom_envs.py           # Entrenamiento en entornos personalizados
â”‚   â”œâ”€â”€ simple_evaluation.py           # EvaluaciÃ³n simple de agentes
â”‚   â”œâ”€â”€ plot_simple_curves.py          # VisualizaciÃ³n de curvas de aprendizaje
â”‚   â””â”€â”€ README.md                       # DocumentaciÃ³n especÃ­fica de Q-Learning
â”‚
â”œâ”€â”€ ğŸ“ dqn/                             # ImplementaciÃ³n Deep Q-Network
â”‚   â”œâ”€â”€ dqn_agent.py                    # Agente DQN con MLP y CNN
â”‚   â”œâ”€â”€ train_cartpole.py              # Entrenamiento en CartPole-v1
â”‚   â”œâ”€â”€ train_breakout.py              # Entrenamiento en Breakout-v0
â”‚   â”œâ”€â”€ train_custom_envs.py           # Entrenamiento en entornos personalizados
â”‚   â”œâ”€â”€ train_cartpole_with_plots.py   # Entrenamiento con visualizaciones
â”‚   â”œâ”€â”€ minatar_wrapper.py             # Wrapper para entornos MinAtar
â”‚   â””â”€â”€ README.md                       # DocumentaciÃ³n especÃ­fica de DQN
â”‚
â”œâ”€â”€ ğŸ“ reinforce/                       # ImplementaciÃ³n REINFORCE
â”‚   â””â”€â”€ reinforce.py                    # Algoritmo REINFORCE completo
â”‚
â”œâ”€â”€ ğŸ“ utils/                           # Utilidades auxiliares
â”‚   â”œâ”€â”€ replay_buffer.py               # Buffer de experiencias circular
â”‚   â””â”€â”€ plotting.py                    # Funciones de visualizaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ experiments/                     # Experimentos y comparaciones
â”‚   â”œâ”€â”€ compare_dqn_vs_reinforce.py    # ComparaciÃ³n DQN vs REINFORCE
â”‚   â””â”€â”€ Captura de pantalla 2025-10-22 a la(s) 3.12.19 p. m..png
â”‚
â”œâ”€â”€ ğŸ“ plots/                           # GrÃ¡ficos generados
â”‚   â”œâ”€â”€ dqn_cartpole_*.png             # GrÃ¡ficos de entrenamiento DQN en CartPole
â”‚   â”œâ”€â”€ dqn_custom_envs_*.png          # GrÃ¡ficos DQN en entornos personalizados
â”‚   â”œâ”€â”€ qlearning_*.png                # GrÃ¡ficos de Q-Learning
â”‚   â””â”€â”€ reinforce_*.png                # GrÃ¡ficos de REINFORCE
â”‚
â”œâ”€â”€ ğŸ“ runs/                            # Logs de TensorBoard
â”‚   â”œâ”€â”€ cartpole/                       # Logs de entrenamiento en CartPole
â”‚   â””â”€â”€ breakout/                       # Logs de entrenamiento en Breakout
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/                     # Modelos entrenados guardados
â”‚   â”œâ”€â”€ dqn_cartpole.pt                # Modelo DQN entrenado en CartPole
â”‚   â””â”€â”€ dqn_breakout.pt                # Modelo DQN entrenado en Breakout
â”‚
â”œâ”€â”€ ğŸ“„ colab_commands.py               # Comandos de ejemplo para Google Colab
â”œâ”€â”€ ğŸ“„ colab_setup.py                  # ConfiguraciÃ³n del entorno en Colab
â”œâ”€â”€ ğŸ“„ install_colab.py                # InstalaciÃ³n de dependencias en Colab
â””â”€â”€ ğŸ“„ uv.lock                         # Lock file de dependencias con uv
```

## DescripciÃ³n Detallada de Archivos y Carpetas

### ğŸ“„ Archivos Principales

- **`main.py`**: Punto de entrada principal con interfaz de lÃ­nea de comandos para ejecutar diferentes experimentos de RL
- **`pyproject.toml`**: ConfiguraciÃ³n moderna del proyecto usando `uv` como gestor de paquetes
- **`requirements.txt`**: Dependencias pip (legacy) para compatibilidad
- **`README.md`**: DocumentaciÃ³n principal del proyecto
- **`IMPLEMENTATION_SUMMARY.md`**: Resumen detallado de la implementaciÃ³n completa
- **`INFORME_COMPLETO.md`**: Informe acadÃ©mico con resultados y anÃ¡lisis
- **`TP2 DQN-1.2.pdf`**: Enunciado original del trabajo prÃ¡ctico

### ğŸ“ envs/ - Entornos Personalizados
Carpeta con entornos de prueba diseÃ±ados para validar algoritmos de RL:

- **`constant_env.py`**: Entorno que siempre devuelve recompensa +1, Ãºtil para verificar que la funciÃ³n de valor aprende correctamente
- **`random_obs_env.py`**: Entorno con observaciones aleatorias para probar robustez de algoritmos
- **`two_step_env.py`**: Entorno minimal de dos estados para debugging y pruebas bÃ¡sicas

### ğŸ“ qlearning/ - Q-Learning Tabular
ImplementaciÃ³n del algoritmo Q-Learning tradicional con tabla de valores:

- **`qlearning_agent.py`**: Agente Q-Learning con polÃ­tica epsilon-greedy y decaimiento de epsilon
- **`train_custom_envs.py`**: Scripts de entrenamiento en entornos personalizados
- **`simple_evaluation.py`**: Herramientas de evaluaciÃ³n de agentes Q-Learning
- **`plot_simple_curves.py`**: VisualizaciÃ³n de curvas de aprendizaje
- **`README.md`**: DocumentaciÃ³n especÃ­fica del algoritmo Q-Learning

### ğŸ“ dqn/ - Deep Q-Network
ImplementaciÃ³n completa del algoritmo DQN siguiendo el paper original de Mnih et al. (2015):

- **`dqn_agent.py`**: Agente DQN con soporte para MLP (CartPole) y CNN (Breakout)
- **`train_cartpole.py`**: Entrenamiento en CartPole-v1 con logging a TensorBoard
- **`train_breakout.py`**: Entrenamiento en Breakout-v5 con preprocesamiento de imÃ¡genes
- **`train_custom_envs.py`**: Entrenamiento DQN en entornos personalizados
- **`train_cartpole_with_plots.py`**: VersiÃ³n con visualizaciones adicionales
- **`minatar_wrapper.py`**: Wrapper para entornos MinAtar
- **`README.md`**: DocumentaciÃ³n detallada del algoritmo DQN

### ğŸ“ reinforce/ - REINFORCE
ImplementaciÃ³n del algoritmo REINFORCE (Policy Gradient):

- **`reinforce.py`**: Algoritmo REINFORCE completo con logging a TensorBoard

### ğŸ“ utils/ - Utilidades Auxiliares
Herramientas compartidas entre diferentes algoritmos:

- **`replay_buffer.py`**: Buffer de experiencias circular para DQN
- **`plotting.py`**: Funciones de visualizaciÃ³n y generaciÃ³n de grÃ¡ficos

### ğŸ“ experiments/ - Experimentos y Comparaciones
Scripts para experimentos comparativos:

- **`compare_dqn_vs_reinforce.py`**: ComparaciÃ³n detallada entre DQN y REINFORCE en CartPole
- **`Captura de pantalla 2025-10-22 a la(s) 3.12.19 p. m..png`**: Captura de resultados

### ğŸ“ plots/ - GrÃ¡ficos Generados
Visualizaciones de resultados de entrenamiento:

- **`dqn_cartpole_*.png`**: GrÃ¡ficos de entrenamiento DQN en CartPole (recompensas, loss, tiempo)
- **`dqn_custom_envs_*.png`**: GrÃ¡ficos DQN en entornos personalizados
- **`qlearning_*.png`**: GrÃ¡ficos de Q-Learning con diferentes hiperparÃ¡metros
- **`reinforce_*.png`**: GrÃ¡ficos de entrenamiento REINFORCE

### ğŸ“ runs/ - Logs de TensorBoard
Logs de entrenamiento para visualizaciÃ³n en TensorBoard:

- **`cartpole/`**: Logs de entrenamiento en CartPole con diferentes algoritmos
- **`breakout/`**: Logs de entrenamiento en Breakout con DQN

### ğŸ“ checkpoints/ - Modelos Entrenados
Modelos guardados despuÃ©s del entrenamiento:

- **`dqn_cartpole.pt`**: Modelo DQN entrenado en CartPole
- **`dqn_breakout.pt`**: Modelo DQN entrenado en Breakout

### ğŸ“„ Archivos de Google Colab
Scripts especÃ­ficos para ejecutar en Google Colab:

- **`colab_commands.py`**: Comandos de ejemplo para entrenamiento en Colab
- **`colab_setup.py`**: ConfiguraciÃ³n del entorno en Colab
- **`install_colab.py`**: InstalaciÃ³n automÃ¡tica de dependencias en Colab

### ğŸ“„ Archivos de ConfiguraciÃ³n
- **`uv.lock`**: Lock file de dependencias generado por `uv`

## InstalaciÃ³n

### MÃ©todo Recomendado (usando uv)

1. Clona el repositorio:
```bash
git clone <url-del-repositorio>
cd TP2_RL
```

2. Instala las dependencias con uv:
```bash
uv sync
```

3. Activa el entorno virtual:
```bash
source .venv/bin/activate
```

### MÃ©todo Alternativo (usando pip)

```bash
pip install -r requirements.txt
```

### Para Google Colab

```bash
# Ejecutar en Colab
!python install_colab.py
```

## Uso

### Ejecutar desde main.py

```bash
# Entrenar DQN en CartPole
uv run main.py --algorithm dqn --env cartpole --train

# Evaluar agente entrenado
uv run main.py --algorithm dqn --env cartpole --test

# Usar Q-Learning en entorno simple
uv run main.py --algorithm qlearning --env two_step --train
```

### Entrenar directamente

```bash
# CartPole con DQN
uv run dqn/train_cartpole.py --episodes 1000

# Breakout con DQN (requiere mucho tiempo)
uv run dqn/train_breakout.py --episodes 10000

# Evaluar modelo entrenado
uv run dqn/train_cartpole.py --eval --model checkpoints/dqn_cartpole.pt

# Q-Learning en entornos personalizados
uv run qlearning/train_custom_envs.py

# ComparaciÃ³n DQN vs REINFORCE
uv run experiments/compare_dqn_vs_reinforce.py
```

### Comandos EspecÃ­ficos por Algoritmo

#### DQN (Deep Q-Network)
```bash
# Entrenamiento bÃ¡sico en CartPole
uv run dqn/train_cartpole.py

# Entrenamiento con parÃ¡metros personalizados
uv run dqn/train_cartpole.py --episodes 1500 --lr 0.001 --epsilon-decay 0.002

# Entrenamiento en Breakout (requiere GPU recomendada)
uv run dqn/train_breakout.py --episodes 2000

# Entrenamiento en entornos personalizados
uv run dqn/train_custom_envs.py
```

#### Q-Learning Tabular
```bash
# Entrenamiento en entornos personalizados
uv run qlearning/train_custom_envs.py

# EvaluaciÃ³n simple
uv run qlearning/simple_evaluation.py

# VisualizaciÃ³n de curvas
uv run qlearning/plot_simple_curves.py
```

#### REINFORCE
```bash
# Entrenamiento REINFORCE en CartPole
uv run reinforce/reinforce.py
```

## Entornos Disponibles

### Entornos EstÃ¡ndar
- **CartPole**: Control clÃ¡sico de equilibrio de poste
- **Breakout**: Juego de Atari con procesamiento de imÃ¡genes

### Entornos Personalizados
- **ConstantEnv**: Observaciones constantes para testing
- **RandomObsEnv**: Observaciones aleatorias para robustez
- **TwoStepEnv**: Entorno minimal de dos estados

## Algoritmos Implementados

### Q-Learning Tradicional
- Tabla de valores Q para espacios discretos
- PolÃ­tica epsilon-greedy
- Decaimiento de epsilon

### Deep Q-Network (DQN)
- AproximaciÃ³n de funciÃ³n Q con redes neuronales
- Replay buffer para estabilidad
- Target network para reducir correlaciones
- Soporte para CNN (Atari) y redes densas (control clÃ¡sico)

## CaracterÃ­sticas

- **Modular**: FÃ¡cil agregar nuevos entornos y algoritmos
- **Configurable**: HiperparÃ¡metros ajustables
- **VisualizaciÃ³n**: GrÃ¡ficos de entrenamiento y anÃ¡lisis
- **Reproducible**: Semillas para resultados consistentes
- **Extensible**: Base para implementar variantes de DQN

## âœ… Estado de ImplementaciÃ³n

**TODAS LAS IMPLEMENTACIONES ESTÃN COMPLETAS:**

- âœ… **Entornos personalizados**: Implementados y funcionales
- âœ… **Q-Learning tabular**: Algoritmo completo con evaluaciÃ³n
- âœ… **DQN con PyTorch**: ImplementaciÃ³n siguiendo paper original
- âœ… **Loops de entrenamiento**: Completos con logging
- âœ… **Funciones de visualizaciÃ³n**: GrÃ¡ficos automÃ¡ticos
- âœ… **IntegraciÃ³n con TensorBoard**: Logging completo
- âœ… **REINFORCE**: Algoritmo Policy Gradient implementado
- âœ… **ExperimentaciÃ³n**: Comparaciones DQN vs REINFORCE

## ğŸ“Š Resultados y GrÃ¡ficos

El proyecto incluye visualizaciones completas de todos los experimentos:

### GrÃ¡ficos de Entrenamiento DQN
- **CartPole**: Curvas de recompensa, loss y tiempo de entrenamiento
- **Breakout**: Progreso de entrenamiento con preprocesamiento de imÃ¡genes
- **Entornos personalizados**: ValidaciÃ³n en entornos de prueba

### GrÃ¡ficos de Q-Learning
- **Curvas de hiperparÃ¡metros**: ComparaciÃ³n de diferentes configuraciones
- **EvaluaciÃ³n de rendimiento**: AnÃ¡lisis de convergencia

### Comparaciones AlgorÃ­tmicas
- **DQN vs REINFORCE**: AnÃ¡lisis comparativo en CartPole
- **MÃ©tricas de rendimiento**: Tiempo de entrenamiento, estabilidad, convergencia

### TensorBoard
Todos los entrenamientos generan logs detallados en `runs/` para visualizaciÃ³n interactiva:
```bash
tensorboard --logdir runs/
```

## Dependencias Principales

- PyTorch >= 2.0.0
- Gymnasium >= 1.0.0
- NumPy >= 1.21.0
- Matplotlib >= 3.5.0
- tqdm >= 4.62.0

## ğŸš€ CaracterÃ­sticas Avanzadas

### Soporte para Google Colab
- Scripts automÃ¡ticos de instalaciÃ³n y configuraciÃ³n
- Comandos optimizados para entornos de Colab
- DetecciÃ³n automÃ¡tica de GPU

### Logging y Monitoreo
- **TensorBoard**: Logging completo de mÃ©tricas de entrenamiento
- **GrÃ¡ficos automÃ¡ticos**: GeneraciÃ³n de visualizaciones
- **Checkpoints**: Guardado automÃ¡tico de modelos

### ExperimentaciÃ³n
- **HiperparÃ¡metros**: ConfiguraciÃ³n flexible de todos los parÃ¡metros
- **Comparaciones**: Scripts para comparar diferentes algoritmos
- **AnÃ¡lisis**: Herramientas de evaluaciÃ³n y visualizaciÃ³n

## ContribuciÃ³n

Este es un proyecto acadÃ©mico completo. Para extensiones futuras:

1. Implementar variantes de DQN (Double DQN, Dueling DQN, etc.)
2. AÃ±adir mÃ¡s entornos de prueba
3. Implementar otros algoritmos de RL (A2C, PPO, etc.)
4. Mejorar la documentaciÃ³n y ejemplos

## Licencia

Proyecto acadÃ©mico para fines educativos.
